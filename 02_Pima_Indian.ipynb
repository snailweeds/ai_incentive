{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 코드 내부에 한글을 사용가능 하게 해주는 부분입니다.\n",
    "\n",
    "# pandas 라이브러리를 불러옵니다.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 피마 인디언 당뇨병 데이터셋을 불러옵니다. 불러올 때 각 컬럼에 해당하는 이름을 지정합니다.\n",
    "df = pd.read_csv('pima-indians-diabetes.csv',\n",
    "               names = [\"pregnant\", \"plasma\", \"pressure\", \"thickness\", \"insulin\", \"BMI\", \"pedigree\", \"age\", \"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pregnant  plasma  pressure  thickness  insulin   BMI  pedigree  age  class\n",
      "0         6     148        72         35        0  33.6     0.627   50      1\n",
      "1         1      85        66         29        0  26.6     0.351   31      0\n",
      "2         8     183        64          0        0  23.3     0.672   32      1\n",
      "3         1      89        66         23       94  28.1     0.167   21      0\n",
      "4         0     137        40         35      168  43.1     2.288   33      1\n"
     ]
    }
   ],
   "source": [
    "# 처음 5줄을 봅니다.\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   pregnant   768 non-null    int64  \n",
      " 1   plasma     768 non-null    int64  \n",
      " 2   pressure   768 non-null    int64  \n",
      " 3   thickness  768 non-null    int64  \n",
      " 4   insulin    768 non-null    int64  \n",
      " 5   BMI        768 non-null    float64\n",
      " 6   pedigree   768 non-null    float64\n",
      " 7   age        768 non-null    int64  \n",
      " 8   class      768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 데이터의 전반적인 정보를 확인해 봅니다.\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         pregnant      plasma    pressure   thickness     insulin         BMI  \\\n",
      "count  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \n",
      "mean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \n",
      "std      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n",
      "50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \n",
      "75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \n",
      "max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n",
      "\n",
      "         pedigree         age       class  \n",
      "count  768.000000  768.000000  768.000000  \n",
      "mean     0.471876   33.240885    0.348958  \n",
      "std      0.331329   11.760232    0.476951  \n",
      "min      0.078000   21.000000    0.000000  \n",
      "25%      0.243750   24.000000    0.000000  \n",
      "50%      0.372500   29.000000    0.000000  \n",
      "75%      0.626250   41.000000    1.000000  \n",
      "max      2.420000   81.000000    1.000000  \n"
     ]
    }
   ],
   "source": [
    "# 각 정보별 특징을 좀더 자세히 출력합니다.\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pregnant  class\n",
      "0           6      1\n",
      "1           1      0\n",
      "2           8      1\n",
      "3           1      0\n",
      "4           0      1\n",
      "..        ...    ...\n",
      "763        10      0\n",
      "764         2      0\n",
      "765         5      0\n",
      "766         1      1\n",
      "767         1      0\n",
      "\n",
      "[768 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 중 임신 정보와 클래스 만을 출력해 봅니다.\n",
    "print(df[['pregnant', 'class']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pregnant</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.342342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.214815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.184466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.338235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.368421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             class\n",
       "pregnant          \n",
       "0         0.342342\n",
       "1         0.214815\n",
       "2         0.184466\n",
       "3         0.360000\n",
       "4         0.338235\n",
       "5         0.368421\n",
       "6         0.320000\n",
       "7         0.555556\n",
       "8         0.578947\n",
       "9         0.642857\n",
       "10        0.416667\n",
       "11        0.636364\n",
       "12        0.444444\n",
       "13        0.500000\n",
       "14        1.000000\n",
       "15        1.000000\n",
       "17        1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['pregnant', 'class']].groupby(['pregnant']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    pregnant     class\n",
      "0          0  0.342342\n",
      "1          1  0.214815\n",
      "2          2  0.184466\n",
      "3          3  0.360000\n",
      "4          4  0.338235\n",
      "5          5  0.368421\n",
      "6          6  0.320000\n",
      "7          7  0.555556\n",
      "8          8  0.578947\n",
      "9          9  0.642857\n",
      "10        10  0.416667\n",
      "11        11  0.636364\n",
      "12        12  0.444444\n",
      "13        13  0.500000\n",
      "14        14  1.000000\n",
      "15        15  1.000000\n",
      "16        17  1.000000\n"
     ]
    }
   ],
   "source": [
    "# 데이터 가공하기 : 임신횟수와 당뇨병 발병 확률\n",
    "print(df[['pregnant', 'class']].groupby(['pregnant'], as_index=False).mean().sort_values(by='pregnant', ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터 간의 상관관계를 그래프로 표현해 봅니다.\n",
    "\n",
    "colormap = plt.cm.gist_heat   #그래프의 색상 구성을 정합니다.\n",
    "plt.figure(figsize=(12,12))   #그래프의 크기를 정합니다.\n",
    "\n",
    "# 그래프의 속성을 결정합니다. vmax의 값을 0.5로 지정해 0.5에 가까울 수록 밝은 색으로 표시되게 합니다.\n",
    "sns.heatmap(df.corr(),linewidths=0.1,vmax=0.5, cmap=colormap, linecolor='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid = sns.FacetGrid(df, col='class')\n",
    "grid.map(plt.hist, 'plasma',  bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "77/77 [==============================] - 1s 2ms/step - loss: 10.5440 - accuracy: 0.6159\n",
      "Epoch 2/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 5.4367 - accuracy: 0.6029\n",
      "Epoch 3/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 2.9292 - accuracy: 0.5208\n",
      "Epoch 4/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 1.5346 - accuracy: 0.5208\n",
      "Epoch 5/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.8896 - accuracy: 0.5000\n",
      "Epoch 6/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.8072 - accuracy: 0.5234\n",
      "Epoch 7/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.7655 - accuracy: 0.6549\n",
      "Epoch 8/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.7306 - accuracy: 0.6628\n",
      "Epoch 9/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6943 - accuracy: 0.6706\n",
      "Epoch 10/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6656 - accuracy: 0.6758\n",
      "Epoch 11/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6390 - accuracy: 0.6797\n",
      "Epoch 12/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6198 - accuracy: 0.6823\n",
      "Epoch 13/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6072 - accuracy: 0.6979\n",
      "Epoch 14/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6112 - accuracy: 0.6966\n",
      "Epoch 15/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5993 - accuracy: 0.7005\n",
      "Epoch 16/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5894 - accuracy: 0.6940\n",
      "Epoch 17/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5915 - accuracy: 0.7057\n",
      "Epoch 18/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5854 - accuracy: 0.7122\n",
      "Epoch 19/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5873 - accuracy: 0.6992\n",
      "Epoch 20/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5820 - accuracy: 0.7070\n",
      "Epoch 21/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5838 - accuracy: 0.6810\n",
      "Epoch 22/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5759 - accuracy: 0.7109\n",
      "Epoch 23/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5752 - accuracy: 0.7148\n",
      "Epoch 24/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5767 - accuracy: 0.7070\n",
      "Epoch 25/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5737 - accuracy: 0.7122\n",
      "Epoch 26/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5664 - accuracy: 0.7161\n",
      "Epoch 27/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5801 - accuracy: 0.7057\n",
      "Epoch 28/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5755 - accuracy: 0.7174\n",
      "Epoch 29/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5654 - accuracy: 0.7161\n",
      "Epoch 30/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5635 - accuracy: 0.7188\n",
      "Epoch 31/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5602 - accuracy: 0.7122\n",
      "Epoch 32/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5683 - accuracy: 0.7109\n",
      "Epoch 33/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5661 - accuracy: 0.7018\n",
      "Epoch 34/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5578 - accuracy: 0.7214\n",
      "Epoch 35/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5563 - accuracy: 0.7305\n",
      "Epoch 36/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5657 - accuracy: 0.7122\n",
      "Epoch 37/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5529 - accuracy: 0.7070\n",
      "Epoch 38/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5537 - accuracy: 0.7227\n",
      "Epoch 39/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5552 - accuracy: 0.7174\n",
      "Epoch 40/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5539 - accuracy: 0.7292\n",
      "Epoch 41/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5523 - accuracy: 0.7253\n",
      "Epoch 42/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5533 - accuracy: 0.7227\n",
      "Epoch 43/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5535 - accuracy: 0.7201\n",
      "Epoch 44/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5433 - accuracy: 0.7240\n",
      "Epoch 45/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5648 - accuracy: 0.7122\n",
      "Epoch 46/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5478 - accuracy: 0.7161\n",
      "Epoch 47/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5480 - accuracy: 0.7266\n",
      "Epoch 48/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5413 - accuracy: 0.7318\n",
      "Epoch 49/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5410 - accuracy: 0.7253\n",
      "Epoch 50/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5396 - accuracy: 0.7318\n",
      "Epoch 51/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5349 - accuracy: 0.7279\n",
      "Epoch 52/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5345 - accuracy: 0.7383\n",
      "Epoch 53/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5354 - accuracy: 0.7240\n",
      "Epoch 54/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5387 - accuracy: 0.7188\n",
      "Epoch 55/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7253\n",
      "Epoch 56/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5412 - accuracy: 0.7174\n",
      "Epoch 57/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5511 - accuracy: 0.7240\n",
      "Epoch 58/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5298 - accuracy: 0.7331\n",
      "Epoch 59/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5324 - accuracy: 0.7344\n",
      "Epoch 60/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5280 - accuracy: 0.7292\n",
      "Epoch 61/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5304 - accuracy: 0.7292\n",
      "Epoch 62/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5373 - accuracy: 0.7214\n",
      "Epoch 63/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5283 - accuracy: 0.7357\n",
      "Epoch 64/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5234 - accuracy: 0.7396\n",
      "Epoch 65/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5239 - accuracy: 0.7383\n",
      "Epoch 66/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5215 - accuracy: 0.7344\n",
      "Epoch 67/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5193 - accuracy: 0.7396\n",
      "Epoch 68/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5241 - accuracy: 0.7357\n",
      "Epoch 69/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5224 - accuracy: 0.7370\n",
      "Epoch 70/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5183 - accuracy: 0.7383\n",
      "Epoch 71/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5164 - accuracy: 0.7344\n",
      "Epoch 72/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5182 - accuracy: 0.7357\n",
      "Epoch 73/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5178 - accuracy: 0.7292\n",
      "Epoch 74/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5187 - accuracy: 0.7396\n",
      "Epoch 75/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5240 - accuracy: 0.7344\n",
      "Epoch 76/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5209 - accuracy: 0.7370\n",
      "Epoch 77/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5177 - accuracy: 0.7331\n",
      "Epoch 78/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5169 - accuracy: 0.7370\n",
      "Epoch 79/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5225 - accuracy: 0.7318\n",
      "Epoch 80/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5092 - accuracy: 0.7435\n",
      "Epoch 81/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5121 - accuracy: 0.7422\n",
      "Epoch 82/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7461\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5171 - accuracy: 0.7344\n",
      "Epoch 84/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5120 - accuracy: 0.7305\n",
      "Epoch 85/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5088 - accuracy: 0.7461\n",
      "Epoch 86/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5046 - accuracy: 0.7513\n",
      "Epoch 87/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5061 - accuracy: 0.7500\n",
      "Epoch 88/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5073 - accuracy: 0.7500\n",
      "Epoch 89/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5080 - accuracy: 0.7474\n",
      "Epoch 90/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.7357\n",
      "Epoch 91/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5293 - accuracy: 0.7305\n",
      "Epoch 92/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5144 - accuracy: 0.7513\n",
      "Epoch 93/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7513\n",
      "Epoch 94/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5034 - accuracy: 0.7461\n",
      "Epoch 95/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7448\n",
      "Epoch 96/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4999 - accuracy: 0.7318\n",
      "Epoch 97/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4978 - accuracy: 0.7552\n",
      "Epoch 98/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5050 - accuracy: 0.7344\n",
      "Epoch 99/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5071 - accuracy: 0.7500\n",
      "Epoch 100/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5004 - accuracy: 0.7513\n",
      "Epoch 101/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5015 - accuracy: 0.7448\n",
      "Epoch 102/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4960 - accuracy: 0.7526\n",
      "Epoch 103/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5054 - accuracy: 0.7409\n",
      "Epoch 104/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4959 - accuracy: 0.7552\n",
      "Epoch 105/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4943 - accuracy: 0.7487\n",
      "Epoch 106/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5006 - accuracy: 0.7526\n",
      "Epoch 107/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5023 - accuracy: 0.7539\n",
      "Epoch 108/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5055 - accuracy: 0.7526\n",
      "Epoch 109/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4918 - accuracy: 0.7487\n",
      "Epoch 110/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5006 - accuracy: 0.7578\n",
      "Epoch 111/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4911 - accuracy: 0.7617\n",
      "Epoch 112/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4916 - accuracy: 0.7500\n",
      "Epoch 113/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4930 - accuracy: 0.7539\n",
      "Epoch 114/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4937 - accuracy: 0.7578\n",
      "Epoch 115/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4932 - accuracy: 0.7565\n",
      "Epoch 116/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4913 - accuracy: 0.7474\n",
      "Epoch 117/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4969 - accuracy: 0.7539\n",
      "Epoch 118/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4920 - accuracy: 0.7474\n",
      "Epoch 119/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5022 - accuracy: 0.7552\n",
      "Epoch 120/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4908 - accuracy: 0.7461\n",
      "Epoch 121/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4847 - accuracy: 0.7526\n",
      "Epoch 122/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4915 - accuracy: 0.7461\n",
      "Epoch 123/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4918 - accuracy: 0.7461\n",
      "Epoch 124/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4932 - accuracy: 0.7487\n",
      "Epoch 125/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4904 - accuracy: 0.7539\n",
      "Epoch 126/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4814 - accuracy: 0.7630\n",
      "Epoch 127/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4960 - accuracy: 0.7578\n",
      "Epoch 128/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.7591\n",
      "Epoch 129/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4992 - accuracy: 0.7331\n",
      "Epoch 130/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4868 - accuracy: 0.7565\n",
      "Epoch 131/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4831 - accuracy: 0.7695\n",
      "Epoch 132/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4854 - accuracy: 0.7552\n",
      "Epoch 133/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4851 - accuracy: 0.7643\n",
      "Epoch 134/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4904 - accuracy: 0.7513\n",
      "Epoch 135/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4940 - accuracy: 0.7500\n",
      "Epoch 136/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.7435\n",
      "Epoch 137/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.7539\n",
      "Epoch 138/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4964 - accuracy: 0.7539\n",
      "Epoch 139/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4841 - accuracy: 0.7578\n",
      "Epoch 140/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4849 - accuracy: 0.7695\n",
      "Epoch 141/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4792 - accuracy: 0.7539\n",
      "Epoch 142/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.7591\n",
      "Epoch 143/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4916 - accuracy: 0.7604\n",
      "Epoch 144/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4814 - accuracy: 0.7578\n",
      "Epoch 145/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4941 - accuracy: 0.7526\n",
      "Epoch 146/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4922 - accuracy: 0.7604\n",
      "Epoch 147/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4797 - accuracy: 0.7513\n",
      "Epoch 148/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4808 - accuracy: 0.7513\n",
      "Epoch 149/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.7578\n",
      "Epoch 150/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4795 - accuracy: 0.7591\n",
      "Epoch 151/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4826 - accuracy: 0.7630\n",
      "Epoch 152/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4801 - accuracy: 0.7513\n",
      "Epoch 153/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4805 - accuracy: 0.7604\n",
      "Epoch 154/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4812 - accuracy: 0.7656\n",
      "Epoch 155/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4752 - accuracy: 0.7591\n",
      "Epoch 156/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4727 - accuracy: 0.7734\n",
      "Epoch 157/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4836 - accuracy: 0.7539\n",
      "Epoch 158/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4739 - accuracy: 0.7682\n",
      "Epoch 159/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4785 - accuracy: 0.7643\n",
      "Epoch 160/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4851 - accuracy: 0.7539\n",
      "Epoch 161/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4796 - accuracy: 0.7695\n",
      "Epoch 162/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4798 - accuracy: 0.7565\n",
      "Epoch 163/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4780 - accuracy: 0.7643\n",
      "Epoch 164/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4776 - accuracy: 0.7695\n",
      "Epoch 165/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4839 - accuracy: 0.7591\n",
      "Epoch 166/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4726 - accuracy: 0.7760\n",
      "Epoch 167/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4675 - accuracy: 0.7721\n",
      "Epoch 168/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4769 - accuracy: 0.7630\n",
      "Epoch 169/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4698 - accuracy: 0.7721\n",
      "Epoch 170/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4785 - accuracy: 0.7630\n",
      "Epoch 171/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4658 - accuracy: 0.7669\n",
      "Epoch 172/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4701 - accuracy: 0.7695\n",
      "Epoch 173/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4748 - accuracy: 0.7695\n",
      "Epoch 174/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4653 - accuracy: 0.7643\n",
      "Epoch 175/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4747 - accuracy: 0.7721\n",
      "Epoch 176/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4696 - accuracy: 0.7669\n",
      "Epoch 177/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4744 - accuracy: 0.7643\n",
      "Epoch 178/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4701 - accuracy: 0.7630\n",
      "Epoch 179/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4757 - accuracy: 0.7643\n",
      "Epoch 180/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4661 - accuracy: 0.7552\n",
      "Epoch 181/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4727 - accuracy: 0.7682\n",
      "Epoch 182/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4830 - accuracy: 0.7708\n",
      "Epoch 183/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4648 - accuracy: 0.7604\n",
      "Epoch 184/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4642 - accuracy: 0.7708\n",
      "Epoch 185/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4665 - accuracy: 0.7643\n",
      "Epoch 186/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4678 - accuracy: 0.7747\n",
      "Epoch 187/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4779 - accuracy: 0.7630\n",
      "Epoch 188/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4773 - accuracy: 0.7643\n",
      "Epoch 189/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4652 - accuracy: 0.7708\n",
      "Epoch 190/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4712 - accuracy: 0.7747\n",
      "Epoch 191/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4654 - accuracy: 0.7721\n",
      "Epoch 192/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4674 - accuracy: 0.7591\n",
      "Epoch 193/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4768 - accuracy: 0.7734\n",
      "Epoch 194/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4770 - accuracy: 0.7682\n",
      "Epoch 195/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4659 - accuracy: 0.7786\n",
      "Epoch 196/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4789 - accuracy: 0.7617\n",
      "Epoch 197/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4738 - accuracy: 0.7695\n",
      "Epoch 198/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4705 - accuracy: 0.7591\n",
      "Epoch 199/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4688 - accuracy: 0.7760\n",
      "Epoch 200/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4664 - accuracy: 0.7760\n",
      "24/24 [==============================] - 1s 3ms/step - loss: 0.4601 - accuracy: 0.7734\n",
      "\n",
      " Accuracy: 0.7734\n"
     ]
    }
   ],
   "source": [
    "# 딥러닝을 구동하는 데 필요한 케라스 함수를 불러옵니다.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# 필요한 라이브러리를 불러옵니다.\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "# 실행할 때마다 같은 결과를 출력하기 위해 설정하는 부분입니다.\n",
    "numpy.random.seed(3)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "# 데이터를 불러 옵니다.\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "X = dataset[:,0:8].astype(float)\n",
    "Y = dataset[:,8].astype(float)\n",
    "\n",
    "# 모델을 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 모델을 실행합니다.\n",
    "model.fit(X, Y, epochs=200, batch_size=10)\n",
    "\n",
    "# 결과를 출력합니다.\n",
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "77/77 [==============================] - 2s 3ms/step - loss: 10.5440 - accuracy: 0.6159\n",
      "Epoch 2/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 5.4367 - accuracy: 0.6029\n",
      "Epoch 3/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.9292 - accuracy: 0.5208\n",
      "Epoch 4/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.5346 - accuracy: 0.5208\n",
      "Epoch 5/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.8896 - accuracy: 0.5000\n",
      "Epoch 6/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.8072 - accuracy: 0.5234\n",
      "Epoch 7/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7655 - accuracy: 0.6549\n",
      "Epoch 8/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.7306 - accuracy: 0.6628\n",
      "Epoch 9/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6943 - accuracy: 0.6706\n",
      "Epoch 10/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6656 - accuracy: 0.6758\n",
      "Epoch 11/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6390 - accuracy: 0.6797\n",
      "Epoch 12/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6198 - accuracy: 0.6823\n",
      "Epoch 13/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6072 - accuracy: 0.6979\n",
      "Epoch 14/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6112 - accuracy: 0.6966\n",
      "Epoch 15/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5993 - accuracy: 0.7005\n",
      "Epoch 16/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5894 - accuracy: 0.6940\n",
      "Epoch 17/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5915 - accuracy: 0.7057\n",
      "Epoch 18/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5854 - accuracy: 0.7122\n",
      "Epoch 19/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5873 - accuracy: 0.6992\n",
      "Epoch 20/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5820 - accuracy: 0.7070\n",
      "Epoch 21/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5838 - accuracy: 0.6810\n",
      "Epoch 22/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5759 - accuracy: 0.7109\n",
      "Epoch 23/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5752 - accuracy: 0.7148\n",
      "Epoch 24/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5767 - accuracy: 0.7070\n",
      "Epoch 25/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5737 - accuracy: 0.7122\n",
      "Epoch 26/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5664 - accuracy: 0.7161\n",
      "Epoch 27/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5801 - accuracy: 0.7057\n",
      "Epoch 28/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5755 - accuracy: 0.7174\n",
      "Epoch 29/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5654 - accuracy: 0.7161\n",
      "Epoch 30/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5635 - accuracy: 0.7188\n",
      "Epoch 31/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5602 - accuracy: 0.7122\n",
      "Epoch 32/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5683 - accuracy: 0.7109\n",
      "Epoch 33/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5661 - accuracy: 0.7018\n",
      "Epoch 34/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5578 - accuracy: 0.7214\n",
      "Epoch 35/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5563 - accuracy: 0.7305\n",
      "Epoch 36/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5657 - accuracy: 0.7122\n",
      "Epoch 37/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5529 - accuracy: 0.7070\n",
      "Epoch 38/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5537 - accuracy: 0.7227\n",
      "Epoch 39/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5552 - accuracy: 0.7174\n",
      "Epoch 40/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5539 - accuracy: 0.7292\n",
      "Epoch 41/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5523 - accuracy: 0.7253: 0s - loss: 0.5102 - accuracy\n",
      "Epoch 42/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5533 - accuracy: 0.7227\n",
      "Epoch 43/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5535 - accuracy: 0.7201\n",
      "Epoch 44/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5433 - accuracy: 0.7240\n",
      "Epoch 45/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5648 - accuracy: 0.7122\n",
      "Epoch 46/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5478 - accuracy: 0.7161\n",
      "Epoch 47/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5480 - accuracy: 0.7266\n",
      "Epoch 48/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5413 - accuracy: 0.7318\n",
      "Epoch 49/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5410 - accuracy: 0.7253\n",
      "Epoch 50/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5396 - accuracy: 0.7318\n",
      "Epoch 51/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5349 - accuracy: 0.7279\n",
      "Epoch 52/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7383\n",
      "Epoch 53/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5354 - accuracy: 0.7240\n",
      "Epoch 54/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5387 - accuracy: 0.7188\n",
      "Epoch 55/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7253\n",
      "Epoch 56/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5412 - accuracy: 0.7174\n",
      "Epoch 57/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5511 - accuracy: 0.7240\n",
      "Epoch 58/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5298 - accuracy: 0.7331\n",
      "Epoch 59/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5324 - accuracy: 0.7344\n",
      "Epoch 60/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5280 - accuracy: 0.7292\n",
      "Epoch 61/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5304 - accuracy: 0.7292\n",
      "Epoch 62/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5373 - accuracy: 0.7214\n",
      "Epoch 63/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5283 - accuracy: 0.7357\n",
      "Epoch 64/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5234 - accuracy: 0.7396\n",
      "Epoch 65/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5239 - accuracy: 0.7383\n",
      "Epoch 66/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5215 - accuracy: 0.7344\n",
      "Epoch 67/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5193 - accuracy: 0.7396\n",
      "Epoch 68/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5241 - accuracy: 0.7357\n",
      "Epoch 69/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5224 - accuracy: 0.7370\n",
      "Epoch 70/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5183 - accuracy: 0.7383\n",
      "Epoch 71/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5164 - accuracy: 0.7344\n",
      "Epoch 72/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5182 - accuracy: 0.7357\n",
      "Epoch 73/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5178 - accuracy: 0.7292\n",
      "Epoch 74/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5187 - accuracy: 0.7396\n",
      "Epoch 75/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5240 - accuracy: 0.7344\n",
      "Epoch 76/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5209 - accuracy: 0.7370\n",
      "Epoch 77/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5177 - accuracy: 0.7331\n",
      "Epoch 78/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5169 - accuracy: 0.7370\n",
      "Epoch 79/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5225 - accuracy: 0.7318\n",
      "Epoch 80/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5092 - accuracy: 0.7435\n",
      "Epoch 81/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5121 - accuracy: 0.7422\n",
      "Epoch 82/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5087 - accuracy: 0.7461\n",
      "Epoch 83/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5171 - accuracy: 0.7344\n",
      "Epoch 84/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5120 - accuracy: 0.7305\n",
      "Epoch 85/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7461\n",
      "Epoch 86/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5046 - accuracy: 0.7513\n",
      "Epoch 87/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5061 - accuracy: 0.7500\n",
      "Epoch 88/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5073 - accuracy: 0.7500\n",
      "Epoch 89/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5080 - accuracy: 0.7474\n",
      "Epoch 90/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.7357\n",
      "Epoch 91/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5293 - accuracy: 0.7305\n",
      "Epoch 92/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5144 - accuracy: 0.7513\n",
      "Epoch 93/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7513\n",
      "Epoch 94/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5034 - accuracy: 0.7461\n",
      "Epoch 95/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7448\n",
      "Epoch 96/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4999 - accuracy: 0.7318\n",
      "Epoch 97/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4978 - accuracy: 0.7552\n",
      "Epoch 98/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5050 - accuracy: 0.7344\n",
      "Epoch 99/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5071 - accuracy: 0.7500\n",
      "Epoch 100/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5004 - accuracy: 0.7513\n",
      "Epoch 101/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5015 - accuracy: 0.7448\n",
      "Epoch 102/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4960 - accuracy: 0.7526\n",
      "Epoch 103/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5054 - accuracy: 0.7409\n",
      "Epoch 104/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4959 - accuracy: 0.7552\n",
      "Epoch 105/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4943 - accuracy: 0.7487\n",
      "Epoch 106/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5006 - accuracy: 0.7526\n",
      "Epoch 107/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5023 - accuracy: 0.7539\n",
      "Epoch 108/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5055 - accuracy: 0.7526\n",
      "Epoch 109/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4918 - accuracy: 0.7487\n",
      "Epoch 110/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5006 - accuracy: 0.7578\n",
      "Epoch 111/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4911 - accuracy: 0.7617\n",
      "Epoch 112/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4916 - accuracy: 0.7500\n",
      "Epoch 113/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4930 - accuracy: 0.7539\n",
      "Epoch 114/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4937 - accuracy: 0.7578\n",
      "Epoch 115/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4932 - accuracy: 0.7565\n",
      "Epoch 116/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4913 - accuracy: 0.7474\n",
      "Epoch 117/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4969 - accuracy: 0.7539\n",
      "Epoch 118/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4920 - accuracy: 0.7474\n",
      "Epoch 119/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5022 - accuracy: 0.7552\n",
      "Epoch 120/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4908 - accuracy: 0.7461\n",
      "Epoch 121/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4847 - accuracy: 0.7526\n",
      "Epoch 122/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4915 - accuracy: 0.7461\n",
      "Epoch 123/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4918 - accuracy: 0.7461\n",
      "Epoch 124/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4932 - accuracy: 0.7487\n",
      "Epoch 125/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4904 - accuracy: 0.7539\n",
      "Epoch 126/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4814 - accuracy: 0.7630\n",
      "Epoch 127/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4960 - accuracy: 0.7578\n",
      "Epoch 128/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.7591\n",
      "Epoch 129/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4992 - accuracy: 0.7331\n",
      "Epoch 130/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4868 - accuracy: 0.7565\n",
      "Epoch 131/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4831 - accuracy: 0.7695\n",
      "Epoch 132/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4854 - accuracy: 0.7552\n",
      "Epoch 133/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4851 - accuracy: 0.7643\n",
      "Epoch 134/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4904 - accuracy: 0.7513\n",
      "Epoch 135/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4940 - accuracy: 0.7500\n",
      "Epoch 136/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.7435\n",
      "Epoch 137/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.7539\n",
      "Epoch 138/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4964 - accuracy: 0.7539\n",
      "Epoch 139/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4841 - accuracy: 0.7578\n",
      "Epoch 140/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4849 - accuracy: 0.7695\n",
      "Epoch 141/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4792 - accuracy: 0.7539\n",
      "Epoch 142/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.7591\n",
      "Epoch 143/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4916 - accuracy: 0.7604\n",
      "Epoch 144/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4814 - accuracy: 0.7578\n",
      "Epoch 145/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4941 - accuracy: 0.7526\n",
      "Epoch 146/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4922 - accuracy: 0.7604\n",
      "Epoch 147/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4797 - accuracy: 0.7513\n",
      "Epoch 148/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4808 - accuracy: 0.7513\n",
      "Epoch 149/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.7578\n",
      "Epoch 150/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4795 - accuracy: 0.7591\n",
      "Epoch 151/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4826 - accuracy: 0.7630\n",
      "Epoch 152/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4801 - accuracy: 0.7513\n",
      "Epoch 153/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4805 - accuracy: 0.7604\n",
      "Epoch 154/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4812 - accuracy: 0.7656\n",
      "Epoch 155/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4752 - accuracy: 0.7591\n",
      "Epoch 156/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4727 - accuracy: 0.7734\n",
      "Epoch 157/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4836 - accuracy: 0.7539\n",
      "Epoch 158/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4739 - accuracy: 0.7682\n",
      "Epoch 159/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4785 - accuracy: 0.7643\n",
      "Epoch 160/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4851 - accuracy: 0.7539\n",
      "Epoch 161/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4796 - accuracy: 0.7695\n",
      "Epoch 162/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4798 - accuracy: 0.7565\n",
      "Epoch 163/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4780 - accuracy: 0.7643\n",
      "Epoch 164/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4776 - accuracy: 0.7695\n",
      "Epoch 165/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4839 - accuracy: 0.7591\n",
      "Epoch 166/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4726 - accuracy: 0.7760\n",
      "Epoch 167/200\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.4675 - accuracy: 0.7721\n",
      "Epoch 168/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4769 - accuracy: 0.7630\n",
      "Epoch 169/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4698 - accuracy: 0.7721\n",
      "Epoch 170/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4785 - accuracy: 0.7630: 0s - loss: 0.4748 - accuracy: \n",
      "Epoch 171/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4658 - accuracy: 0.7669\n",
      "Epoch 172/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4701 - accuracy: 0.7695\n",
      "Epoch 173/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4748 - accuracy: 0.7695\n",
      "Epoch 174/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4653 - accuracy: 0.7643\n",
      "Epoch 175/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4747 - accuracy: 0.7721\n",
      "Epoch 176/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4696 - accuracy: 0.7669\n",
      "Epoch 177/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4744 - accuracy: 0.7643\n",
      "Epoch 178/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4701 - accuracy: 0.7630\n",
      "Epoch 179/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4757 - accuracy: 0.7643\n",
      "Epoch 180/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4661 - accuracy: 0.7552\n",
      "Epoch 181/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4727 - accuracy: 0.7682\n",
      "Epoch 182/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4830 - accuracy: 0.7708: 0s - loss: 0.5016 - accura\n",
      "Epoch 183/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4648 - accuracy: 0.7604\n",
      "Epoch 184/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4642 - accuracy: 0.7708\n",
      "Epoch 185/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4665 - accuracy: 0.7643\n",
      "Epoch 186/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4678 - accuracy: 0.7747\n",
      "Epoch 187/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4779 - accuracy: 0.7630\n",
      "Epoch 188/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4773 - accuracy: 0.7643\n",
      "Epoch 189/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4652 - accuracy: 0.7708\n",
      "Epoch 190/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4712 - accuracy: 0.7747\n",
      "Epoch 191/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4654 - accuracy: 0.7721\n",
      "Epoch 192/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4674 - accuracy: 0.7591\n",
      "Epoch 193/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4768 - accuracy: 0.7734\n",
      "Epoch 194/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4770 - accuracy: 0.7682\n",
      "Epoch 195/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4659 - accuracy: 0.7786\n",
      "Epoch 196/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4789 - accuracy: 0.7617\n",
      "Epoch 197/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4738 - accuracy: 0.7695\n",
      "Epoch 198/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4705 - accuracy: 0.7591\n",
      "Epoch 199/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4688 - accuracy: 0.7760\n",
      "Epoch 200/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4664 - accuracy: 0.7760\n",
      "24/24 [==============================] - 1s 2ms/step - loss: 0.4601 - accuracy: 0.7734\n",
      "\n",
      " Accuracy: 0.7734\n"
     ]
    }
   ],
   "source": [
    "# 딥러닝을 구동하는 데 필요한 케라스 함수를 불러옵니다.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# 필요한 라이브러리를 불러옵니다.\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "# 실행할 때마다 같은 결과를 출력하기 위해 설정하는 부분입니다.\n",
    "numpy.random.seed(3)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "# 데이터를 불러 옵니다.\n",
    "dataset = numpy.loadtxt(\"./dataset/pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "X = dataset[:,0:8].astype(float)\n",
    "Y = dataset[:,8].astype(float)\n",
    "\n",
    "# 모델을 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 모델을 실행합니다.\n",
    "model.fit(X, Y, epochs=200, batch_size=10)\n",
    "\n",
    "# 결과를 출력합니다.\n",
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "77/77 [==============================] - 2s 4ms/step - loss: 10.5440 - accuracy: 0.6159\n",
      "Epoch 2/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 5.4367 - accuracy: 0.6029\n",
      "Epoch 3/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.9292 - accuracy: 0.5208\n",
      "Epoch 4/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5346 - accuracy: 0.5208\n",
      "Epoch 5/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8896 - accuracy: 0.5000\n",
      "Epoch 6/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8072 - accuracy: 0.5234\n",
      "Epoch 7/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7655 - accuracy: 0.6549\n",
      "Epoch 8/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7306 - accuracy: 0.6628\n",
      "Epoch 9/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6943 - accuracy: 0.6706\n",
      "Epoch 10/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6656 - accuracy: 0.6758\n",
      "Epoch 11/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6390 - accuracy: 0.6797\n",
      "Epoch 12/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6198 - accuracy: 0.6823\n",
      "Epoch 13/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6072 - accuracy: 0.6979\n",
      "Epoch 14/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6112 - accuracy: 0.6966\n",
      "Epoch 15/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5993 - accuracy: 0.7005\n",
      "Epoch 16/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5894 - accuracy: 0.6940\n",
      "Epoch 17/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5915 - accuracy: 0.7057\n",
      "Epoch 18/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5854 - accuracy: 0.7122\n",
      "Epoch 19/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5873 - accuracy: 0.6992\n",
      "Epoch 20/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5820 - accuracy: 0.7070\n",
      "Epoch 21/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5838 - accuracy: 0.6810\n",
      "Epoch 22/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5759 - accuracy: 0.7109\n",
      "Epoch 23/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5752 - accuracy: 0.7148\n",
      "Epoch 24/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5767 - accuracy: 0.7070\n",
      "Epoch 25/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5737 - accuracy: 0.7122\n",
      "Epoch 26/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5664 - accuracy: 0.7161\n",
      "Epoch 27/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5801 - accuracy: 0.7057\n",
      "Epoch 28/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5755 - accuracy: 0.7174\n",
      "Epoch 29/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5654 - accuracy: 0.7161\n",
      "Epoch 30/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5635 - accuracy: 0.7188\n",
      "Epoch 31/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5602 - accuracy: 0.7122\n",
      "Epoch 32/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5683 - accuracy: 0.7109\n",
      "Epoch 33/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5661 - accuracy: 0.7018\n",
      "Epoch 34/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5578 - accuracy: 0.7214\n",
      "Epoch 35/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5563 - accuracy: 0.7305\n",
      "Epoch 36/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5657 - accuracy: 0.7122\n",
      "Epoch 37/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5529 - accuracy: 0.7070\n",
      "Epoch 38/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5537 - accuracy: 0.7227\n",
      "Epoch 39/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5552 - accuracy: 0.7174\n",
      "Epoch 40/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5539 - accuracy: 0.7292\n",
      "Epoch 41/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5523 - accuracy: 0.7253\n",
      "Epoch 42/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5533 - accuracy: 0.7227\n",
      "Epoch 43/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5535 - accuracy: 0.7201\n",
      "Epoch 44/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5433 - accuracy: 0.7240\n",
      "Epoch 45/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5648 - accuracy: 0.7122\n",
      "Epoch 46/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5478 - accuracy: 0.7161\n",
      "Epoch 47/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5480 - accuracy: 0.7266\n",
      "Epoch 48/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5413 - accuracy: 0.7318\n",
      "Epoch 49/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5410 - accuracy: 0.7253\n",
      "Epoch 50/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5396 - accuracy: 0.7318: 0s - loss: 0.5569 - accuracy: \n",
      "Epoch 51/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5349 - accuracy: 0.7279\n",
      "Epoch 52/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5345 - accuracy: 0.7383\n",
      "Epoch 53/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5354 - accuracy: 0.7240\n",
      "Epoch 54/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5387 - accuracy: 0.7188\n",
      "Epoch 55/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5336 - accuracy: 0.7253\n",
      "Epoch 56/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5412 - accuracy: 0.7174\n",
      "Epoch 57/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5511 - accuracy: 0.7240\n",
      "Epoch 58/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5298 - accuracy: 0.7331\n",
      "Epoch 59/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7344\n",
      "Epoch 60/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5280 - accuracy: 0.7292\n",
      "Epoch 61/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5304 - accuracy: 0.7292\n",
      "Epoch 62/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5373 - accuracy: 0.7214\n",
      "Epoch 63/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5283 - accuracy: 0.7357\n",
      "Epoch 64/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5234 - accuracy: 0.7396\n",
      "Epoch 65/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5239 - accuracy: 0.7383\n",
      "Epoch 66/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5215 - accuracy: 0.7344\n",
      "Epoch 67/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5193 - accuracy: 0.7396\n",
      "Epoch 68/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5241 - accuracy: 0.7357\n",
      "Epoch 69/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5224 - accuracy: 0.7370\n",
      "Epoch 70/200\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.5149 - accuracy: 0.73 - 0s 3ms/step - loss: 0.5183 - accuracy: 0.7383\n",
      "Epoch 71/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5164 - accuracy: 0.7344\n",
      "Epoch 72/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5182 - accuracy: 0.7357\n",
      "Epoch 73/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5178 - accuracy: 0.7292\n",
      "Epoch 74/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5187 - accuracy: 0.7396\n",
      "Epoch 75/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5240 - accuracy: 0.7344\n",
      "Epoch 76/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5209 - accuracy: 0.7370\n",
      "Epoch 77/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5177 - accuracy: 0.7331\n",
      "Epoch 78/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5169 - accuracy: 0.7370\n",
      "Epoch 79/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5225 - accuracy: 0.7318\n",
      "Epoch 80/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5092 - accuracy: 0.7435\n",
      "Epoch 81/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5121 - accuracy: 0.7422\n",
      "Epoch 82/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5087 - accuracy: 0.7461\n",
      "Epoch 83/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5171 - accuracy: 0.7344\n",
      "Epoch 84/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5120 - accuracy: 0.7305\n",
      "Epoch 85/200\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.5088 - accuracy: 0.7461\n",
      "Epoch 86/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5046 - accuracy: 0.7513\n",
      "Epoch 87/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5061 - accuracy: 0.7500\n",
      "Epoch 88/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5073 - accuracy: 0.7500\n",
      "Epoch 89/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5080 - accuracy: 0.7474\n",
      "Epoch 90/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5111 - accuracy: 0.7357\n",
      "Epoch 91/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5293 - accuracy: 0.7305\n",
      "Epoch 92/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5144 - accuracy: 0.7513\n",
      "Epoch 93/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7513\n",
      "Epoch 94/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5034 - accuracy: 0.7461\n",
      "Epoch 95/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5097 - accuracy: 0.7448\n",
      "Epoch 96/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4999 - accuracy: 0.7318\n",
      "Epoch 97/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4978 - accuracy: 0.7552\n",
      "Epoch 98/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5050 - accuracy: 0.7344\n",
      "Epoch 99/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5071 - accuracy: 0.7500\n",
      "Epoch 100/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5004 - accuracy: 0.7513\n",
      "Epoch 101/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5015 - accuracy: 0.7448\n",
      "Epoch 102/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4960 - accuracy: 0.7526\n",
      "Epoch 103/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5054 - accuracy: 0.7409\n",
      "Epoch 104/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4959 - accuracy: 0.7552\n",
      "Epoch 105/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4943 - accuracy: 0.7487\n",
      "Epoch 106/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5006 - accuracy: 0.7526\n",
      "Epoch 107/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7539\n",
      "Epoch 108/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5055 - accuracy: 0.7526\n",
      "Epoch 109/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4918 - accuracy: 0.7487\n",
      "Epoch 110/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5006 - accuracy: 0.7578\n",
      "Epoch 111/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4911 - accuracy: 0.7617\n",
      "Epoch 112/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4916 - accuracy: 0.7500\n",
      "Epoch 113/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4930 - accuracy: 0.7539\n",
      "Epoch 114/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4937 - accuracy: 0.7578\n",
      "Epoch 115/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4932 - accuracy: 0.7565\n",
      "Epoch 116/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4913 - accuracy: 0.7474\n",
      "Epoch 117/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4969 - accuracy: 0.7539\n",
      "Epoch 118/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4920 - accuracy: 0.7474\n",
      "Epoch 119/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5022 - accuracy: 0.7552\n",
      "Epoch 120/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4908 - accuracy: 0.7461\n",
      "Epoch 121/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4847 - accuracy: 0.7526\n",
      "Epoch 122/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4915 - accuracy: 0.7461\n",
      "Epoch 123/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4918 - accuracy: 0.7461\n",
      "Epoch 124/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4932 - accuracy: 0.7487\n",
      "Epoch 125/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4904 - accuracy: 0.7539\n",
      "Epoch 126/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4814 - accuracy: 0.7630\n",
      "Epoch 127/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4960 - accuracy: 0.7578\n",
      "Epoch 128/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4880 - accuracy: 0.7591\n",
      "Epoch 129/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4992 - accuracy: 0.7331\n",
      "Epoch 130/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4868 - accuracy: 0.7565\n",
      "Epoch 131/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4831 - accuracy: 0.7695\n",
      "Epoch 132/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4854 - accuracy: 0.7552\n",
      "Epoch 133/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4851 - accuracy: 0.7643\n",
      "Epoch 134/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4904 - accuracy: 0.7513\n",
      "Epoch 135/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4940 - accuracy: 0.7500\n",
      "Epoch 136/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4827 - accuracy: 0.7435\n",
      "Epoch 137/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4884 - accuracy: 0.7539\n",
      "Epoch 138/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4964 - accuracy: 0.7539\n",
      "Epoch 139/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4841 - accuracy: 0.7578\n",
      "Epoch 140/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4849 - accuracy: 0.7695\n",
      "Epoch 141/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4792 - accuracy: 0.7539\n",
      "Epoch 142/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4827 - accuracy: 0.7591\n",
      "Epoch 143/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4916 - accuracy: 0.7604\n",
      "Epoch 144/200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4814 - accuracy: 0.7578\n",
      "Epoch 145/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4941 - accuracy: 0.7526\n",
      "Epoch 146/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4922 - accuracy: 0.7604\n",
      "Epoch 147/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4797 - accuracy: 0.7513\n",
      "Epoch 148/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4808 - accuracy: 0.7513\n",
      "Epoch 149/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4880 - accuracy: 0.7578\n",
      "Epoch 150/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4795 - accuracy: 0.7591\n",
      "Epoch 151/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4826 - accuracy: 0.7630\n",
      "Epoch 152/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4801 - accuracy: 0.7513\n",
      "Epoch 153/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4805 - accuracy: 0.7604\n",
      "Epoch 154/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4812 - accuracy: 0.7656\n",
      "Epoch 155/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4752 - accuracy: 0.7591\n",
      "Epoch 156/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4727 - accuracy: 0.7734\n",
      "Epoch 157/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4836 - accuracy: 0.7539\n",
      "Epoch 158/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4739 - accuracy: 0.7682\n",
      "Epoch 159/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4785 - accuracy: 0.7643\n",
      "Epoch 160/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4851 - accuracy: 0.7539\n",
      "Epoch 161/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4796 - accuracy: 0.7695\n",
      "Epoch 162/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4798 - accuracy: 0.7565\n",
      "Epoch 163/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4780 - accuracy: 0.7643\n",
      "Epoch 164/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4776 - accuracy: 0.7695\n",
      "Epoch 165/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4839 - accuracy: 0.7591\n",
      "Epoch 166/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4726 - accuracy: 0.7760\n",
      "Epoch 167/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4675 - accuracy: 0.7721\n",
      "Epoch 168/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4769 - accuracy: 0.7630\n",
      "Epoch 169/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4698 - accuracy: 0.7721\n",
      "Epoch 170/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4785 - accuracy: 0.7630\n",
      "Epoch 171/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4658 - accuracy: 0.7669\n",
      "Epoch 172/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4701 - accuracy: 0.7695\n",
      "Epoch 173/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4748 - accuracy: 0.7695\n",
      "Epoch 174/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4653 - accuracy: 0.7643\n",
      "Epoch 175/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4747 - accuracy: 0.7721\n",
      "Epoch 176/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4696 - accuracy: 0.7669\n",
      "Epoch 177/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4744 - accuracy: 0.7643\n",
      "Epoch 178/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4701 - accuracy: 0.7630\n",
      "Epoch 179/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4757 - accuracy: 0.7643\n",
      "Epoch 180/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4661 - accuracy: 0.7552\n",
      "Epoch 181/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4727 - accuracy: 0.7682\n",
      "Epoch 182/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4830 - accuracy: 0.7708\n",
      "Epoch 183/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4648 - accuracy: 0.7604\n",
      "Epoch 184/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4642 - accuracy: 0.7708\n",
      "Epoch 185/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4665 - accuracy: 0.7643\n",
      "Epoch 186/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4678 - accuracy: 0.7747\n",
      "Epoch 187/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4779 - accuracy: 0.7630\n",
      "Epoch 188/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4773 - accuracy: 0.7643\n",
      "Epoch 189/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4652 - accuracy: 0.7708\n",
      "Epoch 190/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4712 - accuracy: 0.7747\n",
      "Epoch 191/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4654 - accuracy: 0.7721\n",
      "Epoch 192/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4674 - accuracy: 0.7591\n",
      "Epoch 193/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4768 - accuracy: 0.7734\n",
      "Epoch 194/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4770 - accuracy: 0.7682\n",
      "Epoch 195/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4659 - accuracy: 0.7786\n",
      "Epoch 196/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4789 - accuracy: 0.7617\n",
      "Epoch 197/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4738 - accuracy: 0.7695\n",
      "Epoch 198/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4705 - accuracy: 0.7591\n",
      "Epoch 199/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4688 - accuracy: 0.7760\n",
      "Epoch 200/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4664 - accuracy: 0.7760\n",
      "24/24 [==============================] - 1s 2ms/step - loss: 0.4601 - accuracy: 0.7734\n",
      "\n",
      " Accuracy: 0.7734\n"
     ]
    }
   ],
   "source": [
    "# 딥러닝을 구동하는 데 필요한 케라스 함수를 불러옵니다.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# 필요한 라이브러리를 불러옵니다.\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "# 실행할 때마다 같은 결과를 출력하기 위해 설정하는 부분입니다.\n",
    "numpy.random.seed(3)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "# 데이터를 불러 옵니다.\n",
    "dataset = numpy.loadtxt(\"./dataset/pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "X = dataset[:,0:8].astype(float)\n",
    "Y = dataset[:,8].astype(float)\n",
    "\n",
    "# 모델을 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 모델을 실행합니다.\n",
    "model.fit(X, Y, epochs=200, batch_size=10)\n",
    "\n",
    "# 결과를 출력합니다.\n",
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y)[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../dataset/pima-indians-diabetes.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-2d3fb01ebc4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# 데이터를 불러 옵니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../dataset/pima-indians-diabetes.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[0;32m    959\u001b[0m             \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    962\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'encoding'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'bz2'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"+\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             return _file_openers[ext](found, mode=mode,\n\u001b[0m\u001b[0;32m    533\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[0;32m    534\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dataset/pima-indians-diabetes.csv'"
     ]
    }
   ],
   "source": [
    "# 딥러닝을 구동하는 데 필요한 케라스 함수를 불러옵니다.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# 필요한 라이브러리를 불러옵니다.\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "# 실행할 때마다 같은 결과를 출력하기 위해 설정하는 부분입니다.\n",
    "numpy.random.seed(3)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "# 데이터를 불러 옵니다.\n",
    "dataset = numpy.loadtxt(\"../dataset/pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "X = dataset[:,0:8].astype(float)\n",
    "Y = dataset[:,8].astype(float)\n",
    "\n",
    "# 모델을 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 모델을 실행합니다.\n",
    "model.fit(X, Y, epochs=200, batch_size=10)\n",
    "\n",
    "# 결과를 출력합니다.\n",
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y)[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.logspace(0,5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
