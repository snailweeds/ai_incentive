{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴스 기사 분류: 다중 분류 문제\n",
    "\n",
    "이 노트북은 [케라스 창시자에게 배우는 딥러닝](https://tensorflow.blog/%EC%BC%80%EB%9D%BC%EC%8A%A4-%EB%94%A5%EB%9F%AC%EB%8B%9D/) 책의 3장 5절의 코드 예제입니다. 책에는 더 많은 내용과 그림이 있습니다. 이 노트북에는 소스 코드에 관련된 설명만 포함합니다.\n",
    "\n",
    "----\n",
    "\n",
    "이전 섹션에서 완전 연결된 신경망을 사용해 벡터 입력을 어떻게 두 개의 클래스로 분류하는지 보았습니다. 두 개 이상의 클래스가 있을 때는 어떻게 해야 할까요?\n",
    "\n",
    "이 절에서 로이터 뉴스를 46개의 상호 배타적인 토픽으로 분류하는 신경망을 만들어 보겠습니다. 클래스가 많기 때문에 이 문제는 다중 분류의 예입니다. 각 데이터 포인트가 정확히 하나의 범주로 분류되기 때문에 좀 더 정확히 말하면 단일 레이블 다중 분류 문제입니다. 각 데이터 포인트가 여러 개의 범주(가령, 토픽)에 속할 수 있다면 이런 문제는 다중 레이블 다중 분류의 문제가 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로이터 데이터셋\n",
    "\n",
    "1986년에 로이터에서 공개한 짧은 뉴스 기사와 토픽의 집합인 로이터 데이터셋을 사용하겠습니다. 이 데이터셋은 텍스트 분류를 위해 널리 사용되는 간단한 데이터셋입니다. 46개의 토픽이 있으며 어떤 토픽은 다른 것에 비해 데이터가 많습니다. 각 토픽은 훈련 세트에 최소한 10개의 샘플을 가지고 있습니다.\n",
    "\n",
    "IMDB와 MNIST와 마찬가지로 로이터 데이터셋은 케라스에 포함되어 있습니다. 한 번 살펴보죠:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB 데이터셋에서처럼 num_words=10000 매개변수는 데이터에서 가장 자주 등장하는 단어 10,000개로 제한합니다.\n",
    "\n",
    "여기에는 8,982개의 훈련 샘플과 2,246개의 테스트 샘플이 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]),\n",
       "       list([1, 3267, 699, 3434, 2295, 56, 2, 7511, 9, 56, 3906, 1073, 81, 5, 1198, 57, 366, 737, 132, 20, 4093, 7, 2, 49, 2295, 2, 1037, 3267, 699, 3434, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2295, 2, 2, 775, 7, 48, 34, 191, 44, 35, 1795, 505, 17, 12]),\n",
       "       list([1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32, 818, 15, 14, 272, 26, 39, 684, 70, 11, 14, 12, 3886, 18, 180, 183, 187, 70, 11, 14, 102, 32, 11, 29, 53, 44, 704, 15, 14, 19, 758, 15, 53, 959, 47, 1013, 15, 14, 19, 132, 15, 39, 965, 32, 11, 14, 147, 72, 11, 180, 183, 187, 44, 11, 14, 102, 19, 11, 123, 186, 90, 67, 960, 4, 78, 13, 68, 467, 511, 110, 59, 89, 90, 67, 1390, 55, 2678, 92, 617, 80, 1274, 46, 905, 220, 13, 4, 346, 48, 235, 629, 5, 211, 5, 1118, 7, 2, 81, 5, 187, 11, 15, 9, 1709, 201, 5, 47, 3615, 18, 478, 4514, 5, 1118, 7, 232, 2, 71, 5, 160, 63, 11, 9, 2, 81, 5, 102, 59, 11, 17, 12]),\n",
       "       ...,\n",
       "       list([1, 141, 3890, 387, 81, 8, 16, 1629, 10, 340, 1241, 850, 31, 56, 3890, 691, 9, 1241, 71, 9, 5985, 2, 2, 699, 2, 2, 2, 699, 244, 5945, 4, 49, 8, 4, 656, 850, 33, 2993, 9, 2139, 340, 3371, 1493, 9, 2, 22, 2, 1094, 687, 83, 35, 15, 257, 6, 57, 9190, 7, 4, 5956, 654, 5, 2, 6191, 1371, 4, 49, 8, 16, 369, 646, 6, 1076, 7, 124, 407, 17, 12]),\n",
       "       list([1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, 258, 3614, 18, 14, 74, 134, 5131, 18, 88, 2321, 72, 11, 14, 1842, 32, 11, 123, 383, 89, 39, 46, 235, 10, 864, 728, 5, 258, 44, 11, 15, 22, 753, 9, 42, 92, 131, 728, 5, 69, 312, 11, 15, 22, 222, 2, 3237, 383, 48, 39, 74, 235, 10, 864, 276, 5, 61, 32, 11, 15, 21, 4, 211, 5, 126, 1072, 42, 92, 131, 46, 19, 352, 11, 15, 22, 710, 220, 9, 42, 92, 131, 276, 5, 59, 61, 11, 15, 22, 10, 455, 7, 1172, 137, 336, 1325, 6, 1532, 142, 971, 6463, 43, 359, 5, 4, 326, 753, 364, 17, 12]),\n",
       "       list([1, 227, 2406, 91, 2, 125, 2855, 21, 4, 3976, 76, 7, 4, 757, 481, 3976, 790, 5259, 5654, 9, 111, 149, 8, 7, 10, 76, 223, 51, 4, 417, 8, 1047, 91, 6917, 1688, 340, 7, 194, 9411, 6, 1894, 21, 127, 2151, 2394, 1456, 6, 3034, 4, 329, 433, 7, 65, 87, 1127, 10, 8219, 1475, 290, 9, 21, 567, 16, 1926, 24, 4, 76, 209, 30, 4033, 6655, 5654, 8, 4, 60, 8, 4, 966, 308, 40, 2575, 129, 2, 295, 277, 1071, 9, 24, 286, 2114, 234, 222, 9, 4, 906, 3994, 8519, 114, 5758, 1752, 7, 4, 113, 17, 12])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  4,  3, ..., 25,  3, 25], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 4, 1378, 2025, 9, 697, 4622, 111, 8, 25, 109, 29, 3650, 11, 150, 244, 364, 33, 30, 30, 1398, 333, 6, 2, 159, 9, 1084, 363, 13, 2, 71, 9, 2, 71, 117, 4, 225, 78, 206, 10, 9, 1214, 8, 4, 270, 5, 2, 7, 748, 48, 9, 2, 7, 207, 1451, 966, 1864, 793, 97, 133, 336, 7, 4, 493, 98, 273, 104, 284, 25, 39, 338, 22, 905, 220, 3465, 644, 59, 20, 6, 119, 61, 11, 15, 58, 579, 26, 10, 67, 7, 4, 738, 98, 43, 88, 333, 722, 12, 20, 6, 19, 746, 35, 15, 10, 9, 1214, 855, 129, 783, 21, 4, 2280, 244, 364, 51, 16, 299, 452, 16, 515, 4, 99, 29, 5, 4, 364, 281, 48, 10, 9, 1214, 23, 644, 47, 20, 324, 27, 56, 2, 2, 5, 192, 510, 17, 12]),\n",
       "       list([1, 2768, 283, 122, 7, 4, 89, 544, 463, 29, 798, 748, 40, 85, 306, 28, 19, 59, 11, 82, 84, 22, 10, 1315, 19, 12, 11, 82, 52, 29, 283, 1135, 558, 2, 265, 2, 6607, 8, 6607, 118, 371, 10, 1503, 281, 4, 143, 4811, 760, 50, 2088, 225, 139, 683, 4, 48, 193, 862, 41, 967, 1999, 30, 1086, 36, 8, 28, 602, 19, 32, 11, 82, 5, 4, 89, 544, 463, 41, 30, 6273, 13, 260, 951, 6607, 8, 69, 1749, 18, 82, 41, 30, 306, 3342, 13, 4, 37, 38, 283, 555, 649, 18, 82, 13, 1721, 282, 9, 132, 18, 82, 41, 30, 385, 21, 4, 169, 76, 36, 8, 107, 4, 106, 524, 10, 295, 3825, 2, 2476, 6, 3684, 6940, 4, 1126, 41, 263, 84, 395, 649, 18, 82, 838, 1317, 4, 572, 4, 106, 13, 25, 595, 2445, 40, 85, 7369, 518, 5, 4, 1126, 51, 115, 680, 16, 6, 719, 250, 27, 429, 6607, 8, 6940, 114, 343, 84, 142, 20, 5, 1145, 1538, 4, 65, 494, 474, 27, 69, 445, 11, 1816, 6607, 8, 109, 181, 2768, 2, 62, 1810, 6, 624, 901, 6940, 107, 4, 1126, 34, 524, 4, 6940, 1126, 41, 447, 7, 1427, 13, 69, 251, 18, 872, 876, 1539, 468, 9063, 242, 5, 646, 27, 1888, 169, 283, 87, 9, 10, 2, 260, 182, 122, 678, 306, 13, 4, 99, 216, 7, 89, 544, 64, 85, 2333, 6, 195, 7254, 6337, 268, 609, 4, 195, 41, 1017, 2765, 2, 4, 73, 706, 2, 92, 4, 91, 3917, 36, 8, 51, 144, 23, 1858, 129, 564, 13, 269, 678, 115, 55, 866, 189, 814, 604, 838, 117, 380, 595, 951, 320, 4, 398, 57, 2233, 7411, 269, 274, 87, 6607, 8, 787, 283, 34, 596, 661, 5467, 13, 2362, 1816, 90, 2, 84, 22, 2202, 1816, 54, 748, 6607, 8, 87, 62, 6154, 84, 161, 5, 1208, 480, 4, 2, 416, 6, 538, 122, 115, 55, 129, 1104, 1445, 345, 389, 31, 4, 169, 76, 36, 8, 787, 398, 7, 4, 2, 1507, 64, 8862, 22, 125, 2, 9, 2876, 172, 399, 9, 2, 5206, 9, 2, 122, 36, 8, 6642, 172, 247, 100, 97, 6940, 34, 75, 477, 541, 4, 283, 182, 4, 2, 295, 301, 2, 125, 2, 6607, 8, 77, 57, 445, 283, 1998, 217, 31, 380, 704, 51, 77, 2, 509, 5, 476, 9, 2876, 122, 115, 853, 6, 1061, 52, 10, 2, 2, 1308, 5, 4, 283, 182, 36, 8, 5296, 114, 30, 531, 6, 6376, 9, 2470, 529, 13, 2, 2, 58, 529, 7, 2148, 2, 185, 1028, 240, 5296, 1028, 949, 657, 57, 6, 1046, 283, 36, 8, 6607, 8, 4, 2217, 34, 9177, 13, 10, 4910, 5, 4, 141, 283, 120, 50, 2877, 7, 1049, 43, 10, 181, 283, 734, 115, 55, 3356, 476, 6, 2195, 10, 73, 120, 50, 41, 6877, 169, 87, 6607, 8, 107, 144, 23, 129, 120, 169, 87, 33, 2409, 30, 1888, 1171, 161, 4, 294, 517, 23, 2, 25, 398, 9, 2060, 283, 21, 4, 236, 36, 8, 143, 169, 87, 641, 1569, 28, 69, 61, 376, 514, 90, 1249, 62, 2, 13, 4, 2217, 696, 122, 404, 2936, 22, 134, 6, 187, 514, 10, 1249, 107, 4, 96, 1043, 1569, 13, 10, 184, 28, 61, 376, 514, 268, 680, 4, 320, 6, 154, 6, 69, 160, 514, 10, 1249, 27, 4, 153, 5, 52, 29, 36, 8, 6607, 8, 612, 408, 10, 3133, 283, 76, 27, 1504, 31, 169, 951, 2, 122, 36, 8, 283, 236, 62, 641, 84, 618, 2, 22, 8417, 8409, 9, 274, 7322, 399, 7587, 51, 115, 55, 45, 4044, 31, 4, 490, 558, 36, 8, 224, 2, 115, 57, 85, 1655, 2671, 5, 283, 6, 4, 37, 38, 7, 1797, 185, 77, 4446, 4, 555, 298, 77, 240, 2, 7, 327, 652, 194, 8773, 6233, 34, 2, 5463, 4884, 1297, 6, 240, 260, 458, 87, 6, 134, 514, 10, 1249, 22, 196, 514, 4, 37, 38, 309, 213, 54, 207, 8577, 25, 134, 139, 89, 283, 494, 555, 22, 4, 2217, 6, 2172, 4278, 434, 835, 22, 3598, 3746, 434, 835, 7, 48, 6607, 8, 618, 225, 586, 333, 122, 572, 126, 2768, 1998, 62, 133, 6, 2458, 233, 28, 602, 188, 5, 4, 704, 1998, 62, 45, 885, 281, 4, 48, 193, 760, 36, 8, 115, 680, 78, 58, 109, 95, 6, 1732, 1516, 281, 4, 225, 760, 17, 12]),\n",
       "       list([1, 4, 309, 2276, 4759, 5, 2015, 403, 1920, 33, 1575, 1627, 1173, 87, 13, 536, 78, 6490, 399, 7, 2068, 212, 10, 634, 179, 8, 137, 5602, 7, 2775, 33, 30, 1015, 43, 33, 5602, 50, 489, 4, 403, 6, 96, 399, 7, 1953, 3587, 8427, 6603, 4132, 3669, 8180, 7163, 9, 2015, 8, 2, 2, 1683, 791, 5, 740, 220, 707, 13, 4, 634, 634, 54, 1405, 6331, 4, 361, 182, 24, 511, 972, 137, 403, 1920, 529, 6, 96, 3711, 399, 41, 30, 2776, 21, 10, 8491, 2002, 503, 5, 188, 6, 353, 26, 2474, 21, 432, 4, 4234, 23, 3288, 435, 34, 737, 6, 246, 7528, 274, 1173, 1627, 87, 13, 399, 992, 27, 274, 403, 87, 2631, 85, 480, 52, 2015, 403, 820, 13, 10, 139, 9, 115, 949, 609, 890, 819, 6, 812, 593, 7, 576, 7, 194, 2329, 216, 2, 8, 2, 8, 634, 33, 768, 2085, 593, 4, 403, 1920, 185, 9, 107, 403, 87, 2, 107, 1635, 410, 4, 682, 189, 161, 1635, 762, 274, 5319, 115, 30, 43, 389, 410, 4, 682, 107, 1635, 762, 456, 36, 8, 184, 4057, 95, 1854, 107, 403, 87, 302, 2, 8, 129, 100, 756, 7, 3288, 96, 298, 55, 370, 731, 866, 189, 115, 949, 9695, 115, 949, 343, 756, 2, 9, 115, 949, 343, 756, 2509, 36, 8, 17, 12]),\n",
       "       ...,\n",
       "       list([1, 1809, 124, 53, 653, 26, 39, 5439, 18, 14, 5893, 18, 155, 177, 53, 544, 26, 39, 19, 5121, 18, 14, 19, 6382, 18, 280, 3882, 11, 14, 3123, 32, 11, 695, 3614, 47, 11, 14, 3615, 63, 11, 430, 3259, 44, 11, 14, 61, 11, 17, 12]),\n",
       "       list([1, 5586, 2, 71, 8, 23, 166, 344, 10, 78, 13, 68, 80, 467, 606, 6, 261, 5, 146, 93, 124, 4, 166, 75, 3603, 2, 5907, 265, 8692, 1251, 2, 297, 1127, 195, 9, 621, 575, 1080, 5907, 7, 378, 104, 421, 648, 20, 5, 4, 49, 2, 8, 1708, 28, 4, 303, 163, 524, 10, 1220, 6, 455, 4, 326, 685, 6, 2, 422, 71, 142, 73, 863, 62, 75, 3603, 6, 4, 326, 166, 2, 34, 1652, 3603, 6, 4, 166, 4, 49, 8, 17, 12]),\n",
       "       list([1, 706, 209, 658, 4, 37, 38, 309, 484, 4, 1434, 6, 933, 4, 89, 709, 377, 101, 28, 4, 143, 511, 101, 5, 47, 758, 15, 90, 2388, 7, 809, 6, 444, 2035, 4, 911, 5, 709, 198, 1997, 634, 3644, 3798, 2305, 8, 1486, 6, 674, 480, 10, 990, 309, 4008, 2190, 2305, 1849, 24, 68, 583, 242, 5, 4, 143, 709, 364, 7376, 41, 30, 13, 706, 6, 837, 4, 377, 101, 6, 631, 28, 47, 758, 15, 36, 1413, 107, 4, 377, 101, 62, 47, 758, 15, 634, 114, 713, 888, 1412, 6, 343, 37, 38, 1116, 95, 1136, 269, 43, 1488, 1170, 6, 226, 2, 4, 377, 101, 136, 143, 1032, 4, 89, 709, 377, 101, 1217, 30, 478, 97, 47, 948, 15, 90, 4594, 2, 5853, 41, 30, 13, 706, 6, 455, 4, 465, 474, 6, 837, 634, 6, 2069, 4, 709, 377, 101, 28, 47, 758, 15, 7, 463, 29, 89, 1017, 97, 148, 16, 6, 47, 948, 15, 4, 48, 511, 377, 101, 23, 47, 758, 15, 161, 5, 4, 47, 12, 20, 7424, 7978, 386, 240, 2305, 2634, 24, 10, 181, 1475, 7, 194, 534, 21, 709, 364, 756, 33, 30, 4, 386, 404, 36, 118, 4, 2190, 24, 4, 911, 7, 1116, 23, 24, 4, 37, 38, 377, 101, 1976, 42, 9964, 6, 127, 122, 9, 7609, 1136, 692, 13, 37, 38, 1116, 446, 69, 4, 234, 709, 7614, 1320, 13, 126, 1006, 5, 338, 458, 2305, 8, 4, 1136, 911, 23, 4, 307, 2016, 36, 8, 634, 23, 325, 2863, 4, 820, 9, 129, 2767, 40, 836, 85, 1523, 17, 12])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 10,  1, ...,  3,  3, 24], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 245,\n",
       " 273,\n",
       " 207,\n",
       " 156,\n",
       " 53,\n",
       " 74,\n",
       " 160,\n",
       " 26,\n",
       " 14,\n",
       " 46,\n",
       " 296,\n",
       " 26,\n",
       " 39,\n",
       " 74,\n",
       " 2979,\n",
       " 3554,\n",
       " 14,\n",
       " 46,\n",
       " 4689,\n",
       " 4329,\n",
       " 86,\n",
       " 61,\n",
       " 3499,\n",
       " 4795,\n",
       " 14,\n",
       " 61,\n",
       " 451,\n",
       " 4329,\n",
       " 17,\n",
       " 12]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8982,), (8982,))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8982"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2246"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8982,), (8982,))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_data</th>\n",
       "      <th>train_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 3267, 699, 3434, 2295, 56, 2, 7511, 9, 56,...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32,...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 4, 686, 867, 558, 4, 37, 38, 309, 2276, 46...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 8295, 111, 8, 25, 166, 40, 638, 10, 436, 2...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8977</th>\n",
       "      <td>[1, 313, 262, 2529, 1426, 8, 130, 40, 129, 363...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8978</th>\n",
       "      <td>[1, 4, 96, 5, 340, 3976, 23, 328, 6, 154, 7, 4...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8979</th>\n",
       "      <td>[1, 141, 3890, 387, 81, 8, 16, 1629, 10, 340, ...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8980</th>\n",
       "      <td>[1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8981</th>\n",
       "      <td>[1, 227, 2406, 91, 2, 125, 2855, 21, 4, 3976, ...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8982 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             train_data  train_labels\n",
       "0     [1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, ...             3\n",
       "1     [1, 3267, 699, 3434, 2295, 56, 2, 7511, 9, 56,...             4\n",
       "2     [1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32,...             3\n",
       "3     [1, 4, 686, 867, 558, 4, 37, 38, 309, 2276, 46...             4\n",
       "4     [1, 8295, 111, 8, 25, 166, 40, 638, 10, 436, 2...             4\n",
       "...                                                 ...           ...\n",
       "8977  [1, 313, 262, 2529, 1426, 8, 130, 40, 129, 363...            19\n",
       "8978  [1, 4, 96, 5, 340, 3976, 23, 328, 6, 154, 7, 4...            19\n",
       "8979  [1, 141, 3890, 387, 81, 8, 16, 1629, 10, 340, ...            25\n",
       "8980  [1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, ...             3\n",
       "8981  [1, 227, 2406, 91, 2, 125, 2855, 21, 4, 3976, ...            25\n",
       "\n",
       "[8982 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df = pd.DataFrame(train_data)\n",
    "train_labels_df = pd.DataFrame(train_labels)\n",
    "\n",
    "reuters_df = pd.DataFrame(train_data_df)\n",
    "reuters_df.columns=['train_data']\n",
    "reuters_df['train_labels'] = train_labels_df\n",
    "reuters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8982, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8982 entries, 0 to 8981\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   train_data    8982 non-null   object\n",
      " 1   train_labels  8982 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 140.5+ KB\n"
     ]
    }
   ],
   "source": [
    "reuters_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_data      0\n",
       "train_labels    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_data</th>\n",
       "      <th>train_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 3267, 699, 3434, 2295, 56, 2, 7511, 9, 56,...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32,...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 4, 686, 867, 558, 4, 37, 38, 309, 2276, 46...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 8295, 111, 8, 25, 166, 40, 638, 10, 436, 2...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8977</th>\n",
       "      <td>[1, 313, 262, 2529, 1426, 8, 130, 40, 129, 363...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8978</th>\n",
       "      <td>[1, 4, 96, 5, 340, 3976, 23, 328, 6, 154, 7, 4...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8979</th>\n",
       "      <td>[1, 141, 3890, 387, 81, 8, 16, 1629, 10, 340, ...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8980</th>\n",
       "      <td>[1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8981</th>\n",
       "      <td>[1, 227, 2406, 91, 2, 125, 2855, 21, 4, 3976, ...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8982 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             train_data  train_labels\n",
       "0     [1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, ...             3\n",
       "1     [1, 3267, 699, 3434, 2295, 56, 2, 7511, 9, 56,...             4\n",
       "2     [1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32,...             3\n",
       "3     [1, 4, 686, 867, 558, 4, 37, 38, 309, 2276, 46...             4\n",
       "4     [1, 8295, 111, 8, 25, 166, 40, 638, 10, 436, 2...             4\n",
       "...                                                 ...           ...\n",
       "8977  [1, 313, 262, 2529, 1426, 8, 130, 40, 129, 363...            19\n",
       "8978  [1, 4, 96, 5, 340, 3976, 23, 328, 6, 154, 7, 4...            19\n",
       "8979  [1, 141, 3890, 387, 81, 8, 16, 1629, 10, 340, ...            25\n",
       "8980  [1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, ...             3\n",
       "8981  [1, 227, 2406, 91, 2, 125, 2855, 21, 4, 3976, ...            25\n",
       "\n",
       "[8982 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, ...\n",
       "1       [1, 3267, 699, 3434, 2295, 56, 2, 7511, 9, 56,...\n",
       "2       [1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32,...\n",
       "3       [1, 4, 686, 867, 558, 4, 37, 38, 309, 2276, 46...\n",
       "4       [1, 8295, 111, 8, 25, 166, 40, 638, 10, 436, 2...\n",
       "                              ...                        \n",
       "8977    [1, 313, 262, 2529, 1426, 8, 130, 40, 129, 363...\n",
       "8978    [1, 4, 96, 5, 340, 3976, 23, 328, 6, 154, 7, 4...\n",
       "8979    [1, 141, 3890, 387, 81, 8, 16, 1629, 10, 340, ...\n",
       "8980    [1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, ...\n",
       "8981    [1, 227, 2406, 91, 2, 125, 2855, 21, 4, 3976, ...\n",
       "Name: train_data, Length: 8982, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, ...\n",
       "1       [1, 3267, 699, 3434, 2295, 56, 2, 7511, 9, 56,...\n",
       "2       [1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32,...\n",
       "3       [1, 4, 686, 867, 558, 4, 37, 38, 309, 2276, 46...\n",
       "4       [1, 8295, 111, 8, 25, 166, 40, 638, 10, 436, 2...\n",
       "                              ...                        \n",
       "8977    [1, 313, 262, 2529, 1426, 8, 130, 40, 129, 363...\n",
       "8978    [1, 4, 96, 5, 340, 3976, 23, 328, 6, 154, 7, 4...\n",
       "8979    [1, 141, 3890, 387, 81, 8, 16, 1629, 10, 340, ...\n",
       "8980    [1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, ...\n",
       "8981    [1, 227, 2406, 91, 2, 125, 2855, 21, 4, 3976, ...\n",
       "Name: train_data, Length: 8982, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_df.loc[:]['train_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8982"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]),\n",
       "       list([1, 3267, 699, 3434, 2295, 56, 2, 7511, 9, 56, 3906, 1073, 81, 5, 1198, 57, 366, 737, 132, 20, 4093, 7, 2, 49, 2295, 2, 1037, 3267, 699, 3434, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2295, 2, 2, 775, 7, 48, 34, 191, 44, 35, 1795, 505, 17, 12]),\n",
       "       list([1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32, 818, 15, 14, 272, 26, 39, 684, 70, 11, 14, 12, 3886, 18, 180, 183, 187, 70, 11, 14, 102, 32, 11, 29, 53, 44, 704, 15, 14, 19, 758, 15, 53, 959, 47, 1013, 15, 14, 19, 132, 15, 39, 965, 32, 11, 14, 147, 72, 11, 180, 183, 187, 44, 11, 14, 102, 19, 11, 123, 186, 90, 67, 960, 4, 78, 13, 68, 467, 511, 110, 59, 89, 90, 67, 1390, 55, 2678, 92, 617, 80, 1274, 46, 905, 220, 13, 4, 346, 48, 235, 629, 5, 211, 5, 1118, 7, 2, 81, 5, 187, 11, 15, 9, 1709, 201, 5, 47, 3615, 18, 478, 4514, 5, 1118, 7, 232, 2, 71, 5, 160, 63, 11, 9, 2, 81, 5, 102, 59, 11, 17, 12]),\n",
       "       ...,\n",
       "       list([1, 141, 3890, 387, 81, 8, 16, 1629, 10, 340, 1241, 850, 31, 56, 3890, 691, 9, 1241, 71, 9, 5985, 2, 2, 699, 2, 2, 2, 699, 244, 5945, 4, 49, 8, 4, 656, 850, 33, 2993, 9, 2139, 340, 3371, 1493, 9, 2, 22, 2, 1094, 687, 83, 35, 15, 257, 6, 57, 9190, 7, 4, 5956, 654, 5, 2, 6191, 1371, 4, 49, 8, 16, 369, 646, 6, 1076, 7, 124, 407, 17, 12]),\n",
       "       list([1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, 258, 3614, 18, 14, 74, 134, 5131, 18, 88, 2321, 72, 11, 14, 1842, 32, 11, 123, 383, 89, 39, 46, 235, 10, 864, 728, 5, 258, 44, 11, 15, 22, 753, 9, 42, 92, 131, 728, 5, 69, 312, 11, 15, 22, 222, 2, 3237, 383, 48, 39, 74, 235, 10, 864, 276, 5, 61, 32, 11, 15, 21, 4, 211, 5, 126, 1072, 42, 92, 131, 46, 19, 352, 11, 15, 22, 710, 220, 9, 42, 92, 131, 276, 5, 59, 61, 11, 15, 22, 10, 455, 7, 1172, 137, 336, 1325, 6, 1532, 142, 971, 6463, 43, 359, 5, 4, 326, 753, 364, 17, 12]),\n",
       "       list([1, 227, 2406, 91, 2, 125, 2855, 21, 4, 3976, 76, 7, 4, 757, 481, 3976, 790, 5259, 5654, 9, 111, 149, 8, 7, 10, 76, 223, 51, 4, 417, 8, 1047, 91, 6917, 1688, 340, 7, 194, 9411, 6, 1894, 21, 127, 2151, 2394, 1456, 6, 3034, 4, 329, 433, 7, 65, 87, 1127, 10, 8219, 1475, 290, 9, 21, 567, 16, 1926, 24, 4, 76, 209, 30, 4033, 6655, 5654, 8, 4, 60, 8, 4, 966, 308, 40, 2575, 129, 2, 295, 277, 1071, 9, 24, 286, 2114, 234, 222, 9, 4, 906, 3994, 8519, 114, 5758, 1752, 7, 4, 113, 17, 12])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB 리뷰처럼 각 샘플은 정수 리스트입니다(단어 인덱스):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "궁금한 경우를 위해 어떻게 단어로 디코딩하는지 알아보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# 0, 1, 2는 '패딩', '문서 시작', '사전에 없음'을 위한 인덱스이므로 3을 뺍니다\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_newswire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "샘플에 연결된 레이블은 토픽의 인덱스로 0과 45 사이의 정수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 준비\n",
    "\n",
    "이전의 예제와 동일한 코드를 사용해서 데이터를 벡터로 변환합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "# 훈련 데이터 벡터 변환\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# 테스트 데이터 벡터 변환\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레이블을 벡터로 바꾸는 방법은 두 가지입니다. 레이블의 리스트를 정수 텐서로 변환하는 것과 원-핫 인코딩을 사용하는 것입니다. 원-핫 인코딩이 범주형 데이터에 널리 사용되기 때문에 범주형 인코딩이라고도 부릅니다. 원-핫 인코딩에 대한 자세한 설명은 6.1절을 참고하세요. 이 경우 레이블의 원-핫 인코딩은 각 레이블의 인덱스 자리는 1이고 나머지는 모두 0인 벡터입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "# 훈련 레이블 벡터 변환\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "# 테스트 레이블 벡터 변환\n",
    "one_hot_test_labels = to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 예제에서 이미 보았듯이 케라스에는 이를 위한 내장 함수가 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 구성\n",
    "\n",
    "이 토픽 분류 문제는 이전의 영화 리뷰 분류 문제와 비슷해 보입니다. 두 경우 모두 짧은 텍스트를 분류하는 것이죠. 여기에서는 새로운 제약 사항이 추가되었습니다. 출력 클래스의 개수가 2에서 46개로 늘어난 점입니다. 출력 공간의 차원이 훨씬 커졌습니다.\n",
    "\n",
    "이전에 사용했던 것처럼 `Dense` 층을 쌓으면 각 층은 이전 층의 출력에서 제공한 정보만 사용할 수 있습니다. 한 층이 분류 문제에 필요한 일부 정보를 누락하면 그 다음 층에서 이를 복원할 방법이 없습니다. 각 층은 잠재적으로 정보의 병목이 될 수 있습니다. 이전 예제에서 16차원을 가진 중간층을 사용했지만 16차원 공간은 46개의 클래스를 구분하기에 너무 제약이 많을 것 같습니다. 이렇게 규모가 작은 층은 유용한 정보를 완전히 잃게 되는 정보의 병목 지점처럼 동작할 수 있습니다.\n",
    "\n",
    "이런 이유로 좀 더 규모가 큰 층을 사용하겠습니다. 64개의 유닛을 사용해 보죠:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 구조에서 주목해야 할 점이 두 가지 있습니다:\n",
    "\n",
    "* 마지막 `Dense` 층의 크기가 46입니다. 각 입력 샘플에 대해서 46차원의 벡터를 출력한다는 뜻입니다. 이 벡터의 각 원소(각 차원)은 각기 다른 출력 클래스가 인코딩된 것입니다.\n",
    "* 마지막 층에 `softmax` 활성화 함수가 사용되었습니다. MNIST 예제에서 이런 방식을 보았습니다. 각 입력 샘플마다 46개의 출력 클래스에 대한 확률 분포를 출력합니다. 즉, 46차원의 출력 벡터를 만들며 `output[i]`는 어떤 샘플이 클래스 `i`에 속할 확률입니다. 46개의 값을 모두 더하면 1이 됩니다.\n",
    "\n",
    "이런 문제에 사용할 최선의 손실 함수는 `categorical_crossentropy`입니다. 이 함수는 두 확률 분포의 사이의 거리를 측정합니다. 여기에서는 네트워크가 출력한 확률 분포와 진짜 레이블의 분포 사이의 거리입니다. 두 분포 사이의 거리를 최소화하면 진짜 레이블에 가능한 가까운 출력을 내도록 모델을 훈련하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 검증\n",
    "\n",
    "훈련 데이터에서 1,000개의 샘플을 따로 떼어서 검증 세트로 사용하겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:1000].astype(float)\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000].astype(float)\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 20번의 에포크로 모델을 훈련시킵니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 24ms/step - loss: 2.6339 - accuracy: 0.5246 - val_loss: 1.7431 - val_accuracy: 0.6380\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 1.4059 - accuracy: 0.7073 - val_loss: 1.2707 - val_accuracy: 0.7210\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.0245 - accuracy: 0.7793 - val_loss: 1.1018 - val_accuracy: 0.7660\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.8018 - accuracy: 0.8324 - val_loss: 1.0049 - val_accuracy: 0.7900\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6357 - accuracy: 0.8700 - val_loss: 0.9544 - val_accuracy: 0.8010\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.5085 - accuracy: 0.8944 - val_loss: 0.9191 - val_accuracy: 0.8120\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.4072 - accuracy: 0.9176 - val_loss: 0.9174 - val_accuracy: 0.8000\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.3326 - accuracy: 0.9286 - val_loss: 0.9366 - val_accuracy: 0.8150\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.2758 - accuracy: 0.9406 - val_loss: 0.9086 - val_accuracy: 0.8160\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.2338 - accuracy: 0.9455 - val_loss: 0.9447 - val_accuracy: 0.8090\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.2053 - accuracy: 0.9498 - val_loss: 0.9143 - val_accuracy: 0.8220\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.1797 - accuracy: 0.9526 - val_loss: 0.9408 - val_accuracy: 0.8170\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.1612 - accuracy: 0.9553 - val_loss: 1.0109 - val_accuracy: 0.8110\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.1515 - accuracy: 0.9562 - val_loss: 1.0695 - val_accuracy: 0.7900\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.1362 - accuracy: 0.9563 - val_loss: 1.0434 - val_accuracy: 0.7990\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.1301 - accuracy: 0.9569 - val_loss: 1.0594 - val_accuracy: 0.8100\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.1278 - accuracy: 0.9559 - val_loss: 1.0290 - val_accuracy: 0.8110\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.1163 - accuracy: 0.9578 - val_loss: 1.1029 - val_accuracy: 0.8060\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.1156 - accuracy: 0.9583 - val_loss: 1.0917 - val_accuracy: 0.8040\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.1114 - accuracy: 0.9587 - val_loss: 1.1309 - val_accuracy: 0.8030\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실과 정확도 곡선을 그려 보죠:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # 그래프를 초기화합니다\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 모델은 9번째 에포크 이후에 과대적합이 시작됩니다. 9번의 에포크로 새로운 모델을 훈련하고 테스트 세트에서 평가하겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "16/16 [==============================] - 1s 43ms/step - loss: 2.7202 - accuracy: 0.5512 - val_loss: 1.7538 - val_accuracy: 0.6460\n",
      "Epoch 2/9\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 1.4161 - accuracy: 0.7108 - val_loss: 1.3019 - val_accuracy: 0.7180\n",
      "Epoch 3/9\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1.0435 - accuracy: 0.7756 - val_loss: 1.1291 - val_accuracy: 0.7480\n",
      "Epoch 4/9\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.8251 - accuracy: 0.8240 - val_loss: 1.0265 - val_accuracy: 0.7800\n",
      "Epoch 5/9\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.6591 - accuracy: 0.8662 - val_loss: 0.9655 - val_accuracy: 0.8060\n",
      "Epoch 6/9\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.5327 - accuracy: 0.8905 - val_loss: 0.9262 - val_accuracy: 0.8020\n",
      "Epoch 7/9\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4309 - accuracy: 0.9083 - val_loss: 0.8965 - val_accuracy: 0.8130\n",
      "Epoch 8/9\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.3511 - accuracy: 0.9243 - val_loss: 0.8854 - val_accuracy: 0.8190\n",
      "Epoch 9/9\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.2922 - accuracy: 0.9341 - val_loss: 0.8721 - val_accuracy: 0.8310\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9688 - accuracy: 0.7916\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=9,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9688103199005127, 0.7916295528411865]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대략 78%의 정확도를 달성했습니다. 균형 잡힌 이진 분류 문제에서 완전히 무작위로 분류하면 50%의 정확도를 달성합니다. 이 문제는 불균형한 데이터셋을 사용하므로 무작위로 분류하면 19% 정도를 달성합니다. 여기에 비하면 이 결과는 꽤 좋은 편입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18833481745325023"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "test_labels_copy = copy.copy(test_labels)\n",
    "np.random.shuffle(test_labels_copy)\n",
    "float(np.sum(np.array(test_labels) == np.array(test_labels_copy))) / len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 새로운 데이터에 대해 예측하기\n",
    "\n",
    "모델 인스턴스의 `predict` 메서드는 46개 토픽에 대한 확률 분포를 반환합니다. 테스트 데이터 전체에 대한 토픽을 예측해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`predictions`의 각 항목은 길이가 46인 벡터입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 벡터의 원소 합은 1입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 큰 값이 예측 클래스가 됩니다. 즉, 가장 확률이 높은 클래스입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 레이블과 손실을 다루는 다른 방법\n",
    "\n",
    "앞서 언급한 것처럼 레이블을 인코딩하는 다른 방법은 다음과 같이 정수 텐서로 변환하는 것입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 방식을 사용하려면 손실 함수 하나만 바꾸면 됩니다. 코드 3-21에 사용된 손실 함수 `categorical_crossentropy`는 레이블이 범주형 인코딩되어 있을 것이라고 기대합니다. 정수 레이블을 사용할 때는 `sparse_categorical_crossentropy`를 사용해야 합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 손실 함수는 인터페이스만 다를 뿐이고 수학적으로는 `categorical_crossentropy`와 동일합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 충분히 큰 중간층을 두어야 하는 이유\n",
    "\n",
    "앞서 언급한 것처럼 마지막 출력이 46차원이기 때문에 중간층의 히든 유닛이 46개보다 많이 적어서는 안 됩니다. 46차원보다 훨씬 작은 중간층(예를 들면 4차원)을 두면 정보의 병목이 어떻게 나타나는지 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 2.6026 - accuracy: 0.3855 - val_loss: 1.9909 - val_accuracy: 0.5130\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 1.7532 - accuracy: 0.5737 - val_loss: 1.6277 - val_accuracy: 0.5850\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 1.4156 - accuracy: 0.6204 - val_loss: 1.4628 - val_accuracy: 0.6150\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 1.2018 - accuracy: 0.6646 - val_loss: 1.3755 - val_accuracy: 0.6620\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0581 - accuracy: 0.7065 - val_loss: 1.3231 - val_accuracy: 0.6920\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 0.9476 - accuracy: 0.7591 - val_loss: 1.3196 - val_accuracy: 0.7070\n",
      "Epoch 7/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.8609 - accuracy: 0.7835 - val_loss: 1.3349 - val_accuracy: 0.7020\n",
      "Epoch 8/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 0.7939 - accuracy: 0.7924 - val_loss: 1.3324 - val_accuracy: 0.7180\n",
      "Epoch 9/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.7362 - accuracy: 0.8051 - val_loss: 1.3521 - val_accuracy: 0.7080\n",
      "Epoch 10/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 0.6887 - accuracy: 0.8108 - val_loss: 1.3783 - val_accuracy: 0.7070\n",
      "Epoch 11/20\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 0.6479 - accuracy: 0.8196 - val_loss: 1.4079 - val_accuracy: 0.7100\n",
      "Epoch 12/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 0.6111 - accuracy: 0.8274 - val_loss: 1.4300 - val_accuracy: 0.7230\n",
      "Epoch 13/20\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 0.5779 - accuracy: 0.8421 - val_loss: 1.4660 - val_accuracy: 0.7160\n",
      "Epoch 14/20\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 0.5465 - accuracy: 0.8498 - val_loss: 1.5369 - val_accuracy: 0.7180\n",
      "Epoch 15/20\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 0.5210 - accuracy: 0.8542 - val_loss: 1.5817 - val_accuracy: 0.7100\n",
      "Epoch 16/20\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 0.4979 - accuracy: 0.8583 - val_loss: 1.6476 - val_accuracy: 0.7090\n",
      "Epoch 17/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.4736 - accuracy: 0.8643 - val_loss: 1.7119 - val_accuracy: 0.7080\n",
      "Epoch 18/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.4525 - accuracy: 0.8753 - val_loss: 1.7120 - val_accuracy: 0.7090\n",
      "Epoch 19/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 0.4330 - accuracy: 0.8768 - val_loss: 1.8122 - val_accuracy: 0.7120\n",
      "Epoch 20/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 0.4213 - accuracy: 0.8795 - val_loss: 1.8069 - val_accuracy: 0.7160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f06a74ba60>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "검증 정확도의 최고 값은 약 71%로 8% 정도 감소되었습니다. 이런 손실의 대부분 원인은 많은 정보(46개 클래스의 분할 초평면을 복원하기에 충분한 정보)를 중간층의 저차원 표현 공간으로 압축하려고 했기 때문입니다. 이 네트워크는 필요한 정보 대부분을 4차원 표현 안에 구겨 넣었지만 전부는 넣지 못했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추가 실험\n",
    "\n",
    "* 더 크거나 작은 층을 사용해 보세요: 32개 유닛, 128개 유닛 등\n",
    "* 여기에서 두 개의 은닉층을 사용했습니다. 한 개의 은닉층이나 세 개의 은닉층을 사용해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "다음은 이 예제에서 배운 것들입니다.\n",
    "\n",
    "* N개의 클래스로 데이터 포인트를 분류하려면 네트워크의 마지막 `Dense` 층의 크기는 N이어야 합니다.\n",
    "* 단일 레이블, 다중 분류 문제에서는 N개의 클래스에 대한 확률 분포를 출력하기 위해 `softmax` 활성화 함수를 사용해야 합니다.\n",
    "* 이런 문제에는 항상 범주형 크로스엔트로피를 사용해야 합니다. 이 함수는 모델이 출력한 확률 분포와 타깃 분포 사이의 거리를 최소화합니다.\n",
    "* 다중 분류에서 레이블을 다루는 두 가지 방법이 있습니다.\n",
    "    * 레이블을 범주형 인코딩(또는 원-핫 인코딩)으로 인코딩하고 `categorical_crossentropy` 손실 함수를 사용합니다.\n",
    "    * 레이블을 정수로 인코딩하고 `sparse_categorical_crossentropy` 손실 함수를 사용합니다.\n",
    "* 많은 수의 범주를 분류할 때 중간층의 크기가 너무 작아 네트워크에 정보의 병목이 생기지 않도록 해야 합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
